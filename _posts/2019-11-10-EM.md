---
title: "Expectation Maximization"
categories:       
    - Basic
tags:           
    - EM algorithm
    - VI
use_math: true
published: true
last_modified_at: 2019-11-10 11:40:00
---

&nbsp;&nbsp;&nbsp;&nbsp; 이 포스팅에서는 일반적인 EM 알고리즘의 해석과 예시로 분산이 고정된 가우시안 분포가 주어졌을 때 손으로 계산하는 과정을 볼 것이다. 참고 문헌은 Bishop의 Pattern Recognition and Machine Learning과 부경대학교 신봉기 교수님 강의 자료다.
​

&nbsp;&nbsp;&nbsp;&nbsp; EM 알고리즘은 관측 데이터가 주어졌을 때, 잠재변수에 의존하는 확률모델의 최대우도나 최대사후확률을 갖는 파라미터를 반복적으로 추정하는 알고리즘이다. 크게 두 단계로 구성되며, 현재 파라미터 값을 이용하여 로그 우도의 기댓값을 추정하는 Expectation(E) 단계와 이 기댓값을 최대화 하는 새로운 파라미터를 추정하는 Maximization(M)단계를 번갈아 반복적으로 수행한다. 이 때 항상 M단계에서 추정한 파라미터 값은 다음 E단계의 파라미터로 사용된다.  
  
​
&nbsp;&nbsp;&nbsp;&nbsp; 특히나 EM알고리즘은 Variational Bayes(혹은 Variational Inference, VB)와 다르게 각 잠재변수의 사후확률 분포를 직접적으로 추론하지 않고, 잠재변수의 분포를 주변화한 로그 우도의 기댓깂 추정 함수를 사용한다. 이를 $\mathbf{Q}$함수라고 지칭하며, 영어로는 **The expected complete data log likelihood**라고 한다. 보다 명확하게 설명하기 위해 수식을 전개해 보도록 하자.  
  


식(1) : 합의 법칙  
식(2) : $\frac{q(z)}{q(z)} 곱함$  
식(3) : 젠센 부등식(Jensen's inequality)  
식(4) : 로그 성질 이용  
식(5) : $E_{q(z)}[\ln p(x,z)]$  