---
title: "NTU-RGB+D 실험" ## 포스트 제목
category:       
    - HAR based on 3D Skeletons
last_modified_at : 2020-04-14
use_math : true
toc: true
---

# NTU-RGB+D 데이터셋

# NTU-RGB+D 데이터셋 구조

1 skeleton file -> N개 frames [N] : struct -> 1 frame : struct -> meta data -> (25개 한 묶음)joints : struct -> 12개 필드 가짐

Ssss : the setup number
Cccc : the camera ID
Pppp : the subject(performer) ID
Rrrr : the replication number (1 or 2)
Aaaa : action class label

# v1 실험

- 목표 : 
    + 관절간 코사인 유사도 상관관계가 행동인식에서 미치는 영향 분석
    + 데이터셋 다뤄보기

- 기간 : 2020.04.25 ~ 2020.05.08

- 결과물 : 2020 하계 한국정보과학회 KCC 학회 논문

# v1.3 실험

## 현재 진행 상황 간략 요약(상세 기술은 추후 작성)

- 총 60개 클래스에서 혼자만 수행하는 클래스만 분석 : 49개 행동
    + 혼자 행동
    + 물체와 상호작용 있음

- 행동 목록

A1. drink water.
A2. eat meal/snack.
A3. brushing teeth.
A4. brushing hair.
A5. drop.
A6. pickup.
A7. throw.
A8. sitting down.
A9. standing up (from sitting position).
A10. clapping.
A11. reading.
A12. writing.
A13. tear up paper.
A14. wear jacket.
A15. take off jacket.
A16. wear a shoe.
A17. take off a shoe.
A18. wear on glasses.
A19. take off glasses.
A20. put on a hat/cap.
A21. take off a hat/cap.
A22. cheer up.
A23. hand waving.
A24. kicking something.
A25. reach into pocket.
A26. hopping (one foot jumping).
A27. jump up.
A28. make a phone call/answer phone.
A29. playing with phone/tablet.
A30. typing on a keyboard.
A31. pointing to something with finger.
A32. taking a selfie.
A33. check time (from watch).
A34. rub two hands together.
A35. nod head/bow.
A36. shake head.
A37. wipe face.
A38. salute.
A39. put the palms together.
A40. cross hands in front (say stop).
A41. sneeze/cough.
A42. staggering.
A43. falling.
A44. touch head (headache).
A45. touch chest (stomachache/heart pain).
A46. touch back (backache).
A47. touch neck (neckache).
A48. nausea or vomiting condition.
A49. use a fan (with hand or paper)/feeling warm.
----
**two people interaction 제외**

A50. punching/slapping other person.
A51. kicking other person.
A52. pushing other person.
A53. pat on back of other person.
A54. point finger at the other person.
A55. hugging other person.
A56. giving something to other person.
A57. touch other person's pocket.
A58. handshaking.
A59. walking towards each other.
A60. walking apart from each other.

- 원본 3D skeleton 데이터 좌표에 대한 전처리 없이 카메라 좌표 그대로 LSTM + FC 수행
    + number of features : 75 dim
    + sequence length : 88 frames (전체 프레임 길이의 평균)
    + hidden layers : 150
    + 성능 : 39% / top-5 : 55%
    + 은닉 계층 개수가 증가하니 성능이 더 오르고 있어 추가 필요
    + Adam -> ASGD시 성능 비약적 증가 => 이유?
    + batch normalization 미수행
    + LSTM dropout 미수행
    + 특징점 개수 PCA로 조절 가능 (Incremental PCA : 카루엔-뢰브 변환)

- temporal / spatial cosine similarity -> LSTM + FC 수행
    + 위와 동일 구조 네트워크
    + number of features : 325 dim
        * 각 25개의 관절 각각 대응하여 25x25 코사인 유사도 행렬에서 상삼각행렬만 추출
    + sequence length : 88 frames
    + hidden layers : 100
    + 성능 :
        * temporal : 30.4% / top-5 : 51.2%
        * spatial : 8.8% / top=5 : 18%
    + ASGD 최적화기 수행
    + 은닉계층 150개시, 학습이 되지 않는 것으로 보임. loss율 변화가 전혀 없음
    + batch normalization 미수행
    + LSTM dropout 미수행
    + 특징점 개수 PCA로 조절 필요해보임

## v1.3 변화

- 현재 문제점 및 개선방향
    1. 원본 데이터가 카메라뷰 좌표게 => 몸 중심 좌표계 변환 필요
    2. 특징점 개수가 너무 많아서 은닉계층이 더 필요 => 차원축소 (overfitting 방지)
    3. 물체로 잘못된 조인트 짚음 => 처리?

- 최종 Two-stream LSTM 구상
    + 1st : origin + spatial features
    + 2nd : temporal features

- 비교 성능 구성
    1. origin 3D skeletons / spatial / temporal 각 성능
    2. origin 3D skeletons + spatial
    3. Two-stream : origin 3D skeletons / temporal
    4. Two-stream : origin 3D skeletons + spatial / temporal

- 해결 기법
    + Karhunen-Loeve transform(KLT) 적용 : sklearn Incremental PCA
    + batch normalization
    + LSTM dropout
    + stacked LSTM
    + 기저 변환
    + 오류 프레임 처리 : 원본 논문 참조?