---
title:          "[네이버 부스트캠프 AI Tech 6기] Generative Model(1)"
categories:       
    - Naver_BoostCamp_AI
tags:           
    - Generative Model
    - Probability Theory
    - Conditional Independence
comments: true
last_modified_at : 2023-11-24 20:44:01
toc: true
---

## 1. Generative Model 
- 개나 고양이 같이 특정 물체가 들어간 사진을 가지고 우리가 원하는 대로 새로운 개 또는 고양이 사진을 생성하고 싶다!
- 일반적으로 사람들이 새로운 것을 생각할 때 본인이 이때까지 봐온 개 나 고양이를 토대로 상상하면서 원하는 형태의 개 나 고양이를 상상할 것이다.
- 그럼 **이때까지 봐온** 의 풀(pool)은 어디서 찾을 수 있을까? 풀을 찾을 수 있다면 컴퓨터에서도 쉽게 넣어준 데이터에서 새로운 사진을 만들 수 있지 않을까?
- 이렇게 사람과 같이 컴퓨터도 이런 경험치를 가진 풀을 알고 그것을 토대로 새로운 사진을 생성해낼 수 있으면 좋겠다. 라는 동기를 가진 것이 생성모델(Generative Model)이다.
- 그래서 가장 먼저, **과거의 경험을 충분히 표현할 수 있는** 확률 분포(풀)인 $$p(x)$$ 를 찾을 것이다. 
- 확률 분포로 표현하는 이유는 수 많은 사진들 중 이 사진이 개 나 고양이 사진일 확률이 얼마인가를 부드럽게 추정할 수 있기 때문이다.
- 만약, 개일때만 1, 다른 동물일때는 0인 함수를 이용한다면, 고양이 같이 생긴 강아지나 강아지 같이 생긴 고양이는 어떻게 구별할 수 있는가?
- 일반적으로 사람이 사고할 때에도 그간의 경험으로 이 동물이 개인지 고양이인지 어떤 점이 다른지 ***추정***할 뿐이다.
- 그러니 우리는 우리가 보고 들은 경험치 풀을 바탕으로 부드럽게 판별하는 것이지 항상 정답을 내릴 수 있는 것은 아니다. 
- 각설하고, 컴퓨터가 입력으로 받은 데이터를 표현할 수 있는 확률 분포, 다른 말로 과거의 경험치 풀은 다음과 같이 표현된다.
- Generation : If we sample 

$$ \tilde{x} \sim p(x) $$ 

$$\tilde{x}$$ should look like a dog or cat
- 여기서 확률 분포 $$p(x)$$ 는 x가 고양이일때만 결과값(확률)이 높을 것이고, 이외 다른 동물들은 낮을 것이다. (확률 분포니까 0~1 사이 값으로 나올 것임)
- 이런 확률 분포는 확률 분포라고 부를 수 있는 이유가 아래 조건들을 만족하기 때문이다.
	1. 강아지가 들어왔을 때 (분포에서 원하는) 높은 확률로 표현이 가능하다.
	2. 모든 입력이 0보다 크고, 1보다 작은 값을 가진다.
	3. 모든 가능한 입력으로 접근했을 때 확률합이 1이 된다.
- 그럼 이런 확률 분포 $$p(x)$$ 를 어떻게 표현할 수 있을까?

---

## 2. 확률 분포는 어떻게 표현할까?
- 위에서 확률 분포 $$p(x)$$를 사용하는 이유와 이 개념을 도입할 수 있는 이유에 대해서도 설명했다.
- 그렇다면 이제 우리가 표현가능한 몇 가지 분포의 종류를 정리하고, 각 분포들을 표현할때 몇 개의 파라미터(모수)가 필요한 지 알아보겠다.
	+ 여기서 말하는 파라미터란, 분포를 수식적으로 나타낼 때 필요한 변수를 말한다.
### 1) Bernoulli distribution (베르누이 분포)
![[./assets/images/custom/Bernoulli_dist.png]]
(Wikipedia Bernoulli distribution 발췌: ![https://en.wikipedia.org/wiki/Bernoulli_distribution])
- 베르누이 분포는 위의 그림과 같이 0 과 1만을 표현하는 분포이다. 좀 더 쉽게, 동전을 던질 때 나올 수 있는 확률 분포이다.
- 일반적으로 양 쪽 면이 동일하게 평평한 동전을 던지면 (주변환경에 대한 요소는 제외하고) 앞 또는 뒷면이 나올 확률은 0.5로 반반이다.
- 한쪽으로 약간 구부러져 한 쪽만 더 잘나오는 상태라면 10번 던져서 앞면이 7번 나오고, 뒷면이 3번 나올 수도 있다. 
- 이런 2가지 경우가 나올 확률들을 표현할 수 있는 분포가 바로 베르누이 분포이다. 수식으로 정의하면 아래와 같다.

$$ D = {Head, Tails} $$ <br>

Specify 

$$P(X = Heads) = p $$$$ P(X = Tails) = 1 -p $$

Write 

$$ X \sim Ber(p) $$

- 여기서 마지막 수식을 (확률)변수 X가 나올 확률이 p인 베르누이 분포를 따른다 라고 한다.
- 이런 베르누이 분포에서 우리는 총 2가지 경우를 얻을 수 있고(앞, 뒷면), 확률 p 1개로 2가지 경우를 모두 표현할 수 있다.
- Head 일때는 p, Tail 일때는 1 - p로 표현할 수 있기 때문이다.

### 2) Categorical distribution (카테고리 분포)
- 이번에는 6면체 주사위를 던진다고 생각해보자.
- 동전과 같이 주사위에 특별한 이상이 없고 모든 면이 동일하게 평평한 조건이라면, 1 ~ 6까지 각 면이 나올 수 있는 확률은 1/6이다.
- 카테고리 분포는 이렇게 m개의 경우를 표현할 수 있는 확률분포이다. 수식으로 정의하면 아래와 같다.

$$ D = {1, ..., m} $$

Specify 

$$ P(Y=i) = p_i $$ 

such that, 

$$ \sum_{i=1}^m p_i = 1 $$

Write 

$$ Y \sim Cat(p1, ..., p_m) $$

- 베르누이 분포와 마찬가지로 마지막 수식은 확률변수 Y가 각각 $$p_1, ..., p_m$$ 확률을 갖는 카테고리 분포를 따른다 라고 한다.
- 카테고리 분포는 단순히 베르누이 분포에서 표현할 수 있는 변수의 경우의 수가 m+1개로 늘어났다.
- 그래서 이 분포에서 총 m+1가지 경우를 얻을 수 있고, p가 m개 있으면 된다.
- 베르누이 분포와 마찬가지로 p가 m개만 있으면 되는 이유는 마지막 경우를 표현할 확률은 앞서 나온 확률들을 다 더한 값에 1만 빼주면 되기 때문이다.

$$ p_{m+1} = 1 - \sum_{i=1}^m p_i $$

### 3) Normal distribution (정규분포)
- 위의 분포들은 이산값만을 나타내는 분포들이었다. 이번에는 연속적인 값을 나타내는 정규분포에 대해 알아보자.
- 정규분포는 19세기 위대한 수학자로 알려진 Carl Friedrich Gauss에 의해 만들어져 가우시안 분포(Gaussian distribution)라고도 불린다.
- 정규분포는 아래 그림과 같이 좌우 대칭인 종모양을 갖는게 특징이다.
![[./assets/images/custom/norm_dist.png]]
(wikipedia Normal distribution 발췌: ![https://en.wikipedia.org/wiki/Normal_distribution])
- 정규분포는 통계학에서 다음 3가지 이유 때문에 아주아주 중요한 분포이다.
	1. 실험이나 관찰을 통해 수집된 데이터들의 확률분포가 대부분 좌우 대칭이고, 종 모양 분포를 가짐을 통계학의 유구한 역사 속에서 알 수 있었다. 따라서 이러한 모양을 가진 정규 분포가 이 현실 데이터들을 표현하는데 널리 쓰이고 있다. 그래서 정규 분포를 따르지 않는 분포를 제곱근이나 세제곱근, 로그등을 취하여 변환해서 정규분포에 근사 시켜 많이 사용한다.
	2. 정규 분포를 표현하는데 필요한 파라미터는 평균(mean)과 분산(standard deviation)으로 단 2개다! 따라서 수학적으로 계산하기도 편하며, 다른 분포들과 긴밀한 관계를 맺고 있다.
	3. 표본의 크기가 커질 경우, 중심극한정리에 따라 정규분포를 따르지 않는 집단에서 추출한 표본들을 모아도 그 평균의 분포는 정규분포를 따르게 됨이 알려져있다. 좀 더 쉽게 말하면, 정규분포를 따르지 않는 분포에서 표본을 하나씩 수집해서 다 모아 그 표본들을 평균내고, 다시 뽑아서 평균내고 반복하여 표본들의 평균 분포인 표본평균 분포가 정규분포와 형태가 같아진다.
- 정규분포의 수식을 살펴보면 아래와 같다.

$$ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{1}{2}\frac{(x-\mu)^2}{\sigma}}$$

$$ X \sim N(\mu,\sigma^2)$$

- 수식이 복잡해 보이지만 자세히 보면 변수라고 할 수 있는 부분이 $$\mu$$와 $$\sigma$$ 뿐인 것을 알 수 있다.
- 따라서 정규분포는 연속된 여러 값을 얻을 수 있고, 이를 표현할 수 있는 파라미터로 단 2개, 평균인 $$\mu$$ 와 표준편차인 $$\sigma$$만 있으면 된다!

---

## 3. Conditional Independent
- 이제 우리는 생성모델을 만들기 위해 분포는 어떤 것인지, 분포를 만들기 위해 파라미터가 필요하다는 것과 필요한 파라미터가 분포별로 다른 것을 알았다. 그럼 이미지를 생성하는 모델을 만들기 위해서는 어떤 분포와 파라미터가 필요할까?
- 먼저, 이미지는 픽셀들의 모음이다. 이런 픽셀들이 각각 RGB색깔을 가지고 잘 조합되야 아래와 같이 예쁜 사진을 얻을 수 있다.
![[./assets/images/custom/IMG_8657.jpeg]]
(사진 출처 : 싱가포르->한국 비행기에서 직접 찍은 사진입니다!)

- 결국 모든 픽셀들이 서로 관련이 있다는 것을 알 수 있다. 위의 사진에서 보면 비행기 날개 부분을 보면 픽셀들이 아래 구름들과 다르게 직선으로 이어져 날개를 표현하고 있음을 볼 수 있다. 따라서, 한 장의 이미지를 얻기 위해서는 모든 픽셀들의 조합이 필요하다.
- 즉, 우리가 일반적으로 찍는 FHD 사진 1장은 1980 x 1080 개의 픽셀들의 각각 RGB, 3개 채널을 조합한 것임을 알 수 있다.
- 따라서, 한 픽셀을 $x$ 라고 할때, 사진 한 장을 표현하는 분포는 아래와 같이 표현할 수 있다.

$$P(x_1, x_2, ..., x_{1920 \times 1080})$$

- 여기서 각 픽셀은 총 3가지 색깔(Red, Green, Blue)를 가질 수 있으니, $P(x_i)$ 가 표현할 수 있는 경우의 수가 3가지이다. 그럼 FHD 사진 1장은 아래 경우의 수들 중에 만들어지는 것이다.

$$ 3^{1920 \times 1080}$$

- 픽셀은 연속적인 값이 아니라 이산적인 값이므로, 2절 예제에서 보았듯, 이산 확률 분포의 파라미터는 분포가 가질 수 있는 모든 경우의 수에서 -1만 하면 되므로, FHD 사진 1장을 표현하기 위한 파라미터 개수는 아래와 같이 어마어마한 수가 된다. 

$$3^{1920 \times 1080} - 1$$

- 이게 과연 우리가 만들 생성모델이 가질 수 있는 분포가 될 수 있을까? 정답은 No이다. 사진 1장을 만들기 위해 저만한 파리미터들을 조정 하면서 원하는 사진을 탐색하다간 죽기전에도 못 볼 것이다.
- 그렇다면 사진에서도 보이듯이 비행기 날개를 표현하는 영역, 해를 표현하는 영역, 구름을 표현하는 영역이 나눠져 있으니 픽셀들 모두가 상관있다고 하지 않고, 어느정도 독립적이라고 생각할 수는 없을까?

### Three important rules
- 확률론에서 변수들의 독립을 가정한 3가지 중요한 법칙이 있다.
1. Chain rule
	- Chain rule은 1부터 n까지 데이터의 조합에서 각 데이터는 자신 이전에 들어온 과거 데이터에서만 영향을 받는 규칙이다. 주로 다음에 나올 단어나 내일의 날씨 예측과 같이 과거의 데이터들이 현재에도 영향을 미칠 수 있는 sequence 데이터에서 많이 사용된다.

	$$p(x_1, ..., x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)...p(x_n|x_1,...,x_{n-1})$$

2. Bayes' rule
	- 베이즈 정리는 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리이다. 먼저 수식으로 나타내면 아래와 같다.

	$$p(x|y) = \frac{p(x,y)}{p(y)} = \frac{p(y|x)p(x)}{p(y)}$$
	
    - 사전 확률(prior probability), 사후 확률(posterior probability) 그리고 가능도(likelihood, 우도)에 대해 쉽게 설명하면 다음과 같다.
		- 사전 확률 : 친구가 아이스크림을 사러 올 때, 친구가 이미 좋아하는 아이스크림의 확률이다. 예를 들어 친구가 바닐라 아이스크림을 좋아한다면, P(바닐라) 가 사전 확률이다.
		- 가능도 : 아이스크림 가게를 다녀온 친구의 아이스크림이 바닐라 아이스크림일 가능성! 을 나타내는 확률이다. 이를 P(바닐라|가게에 다녀옴) 라고 표현한다.
		- 사후 확률 : 친구가 사온 아이스크림이 바닐라일 확률을 계



---

## 참고 문헌
- 정규분포의 중요성 : ![https://blog.naver.com/definitice/220950767553]
- 베이즈 정리 : ![https://ko.wikipedia.org/wiki/베이즈_정리#:~:text=확률론과%20통계학에서%20베이,확률을%20구할%20수%20있다.]
